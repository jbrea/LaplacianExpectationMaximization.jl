<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Home · LaplacianExpectationMaximization.jl</title><meta name="title" content="Home · LaplacianExpectationMaximization.jl"/><meta property="og:title" content="Home · LaplacianExpectationMaximization.jl"/><meta property="twitter:title" content="Home · LaplacianExpectationMaximization.jl"/><meta name="description" content="Documentation for LaplacianExpectationMaximization.jl."/><meta property="og:description" content="Documentation for LaplacianExpectationMaximization.jl."/><meta property="twitter:description" content="Documentation for LaplacianExpectationMaximization.jl."/><script data-outdated-warner src="assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="assets/documenter.js"></script><script src="search_index.js"></script><script src="siteinfo.js"></script><script src="../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href>LaplacianExpectationMaximization.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li class="is-active"><a class="tocitem" href>Home</a><ul class="internal"><li><a class="tocitem" href="#Example"><span>Example</span></a></li></ul></li><li><a class="tocitem" href="rl/">Fitting a Q-Learner</a></li><li><a class="tocitem" href="reference/">Reference</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Home</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Home</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/jbrea/LaplacianExpectationMaximization.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/jbrea/LaplacianExpectationMaximization.jl/blob/main/docs/src/index.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="LaplacianExpectationMaximization"><a class="docs-heading-anchor" href="#LaplacianExpectationMaximization">LaplacianExpectationMaximization</a><a id="LaplacianExpectationMaximization-1"></a><a class="docs-heading-anchor-permalink" href="#LaplacianExpectationMaximization" title="Permalink"></a></h1><p>This package implements the Expectation-Maximization (EM) algorithm with a Laplacian approximation for the E-step, as described e.g. <a href="http://dx.doi.org/10.1371/journal.pcbi.1002410">here</a>, for finding the maximum likelihood estimate of the parameter distribution of a model.</p><p>Assume some data <span>$\mathcal D = \{\boldsymbol x_1, \ldots, \boldsymbol x_n\}$</span> was generated by sampling independently from the conditional density <span>$\boldsymbol x_i\sim p(\boldsymbol x|\boldsymbol \theta_i)$</span>, where <span>$\boldsymbol \theta_i$</span> are parameters sampled from a multivariate normal distribution with diagonal covariance <span>$\boldsymbol \theta_i\sim\mathcal N(\boldsymbol \theta; \boldsymbol \mu, \boldsymbol\sigma)$</span>. This package allows to find the maximum likelihood estimates </p><p class="math-container">\[\begin{align*}
\boldsymbol \mu^*, \boldsymbol \sigma^* = \arg\max_{\boldsymbol \mu, \boldsymbol\sigma}p(\mathcal D | \boldsymbol\mu, \boldsymbol \sigma) &amp;= \arg\max_{\boldsymbol \mu, \boldsymbol\sigma}\sum_{i=1}^n\log(p(\boldsymbol x_i|\boldsymbol\mu, \boldsymbol\sigma))\\ &amp;= \arg\max_{\boldsymbol \mu, \boldsymbol\sigma}\sum_{i=1}^n\log\left(\int p(\boldsymbol x_i|\boldsymbol \theta_i)\mathcal N(\boldsymbol\theta_i;\boldsymbol\mu, \boldsymbol\sigma)d\boldsymbol\theta_i\right)
\end{align*}\]</p><p>using the EM algorithm with Laplacian approximation for the E-step.</p><h2 id="Example"><a class="docs-heading-anchor" href="#Example">Example</a><a id="Example-1"></a><a class="docs-heading-anchor-permalink" href="#Example" title="Permalink"></a></h2><h3 id="Model-Definition"><a class="docs-heading-anchor" href="#Model-Definition">Model Definition</a><a id="Model-Definition-1"></a><a class="docs-heading-anchor-permalink" href="#Model-Definition" title="Permalink"></a></h3><p>For a sequence of binary values <span>$y = (y_1, \ldots, y_T)$</span> we define a habituating biased coin model with probability <span>$P(y) = \prod_{t=1}^TP(y_t|w_{t-1})$</span> with <span>$w_t = w_{t-1} + \eta (y_{t-1} - \sigma(w_{t-1}))$</span>, where <span>$w_0$</span> and <span>$\eta$</span> are parameters of the model and <span>$\sigma(w) = 1/(1 + \exp(-w))$</span>.</p><p>We define the model <code>HabituatingBiasedCoin</code> with state variable <code>w</code> and extend the functions <code>parameters, initialize!, logp</code> and <code>sample</code> from <code>LaplacianExpectationMaximization</code>.</p><pre><code class="language-julia hljs"># import LaplacianExpectationMaximization: parameters, initialize!, logp, sample # if you want to avoid writing LaplacianExpectationMaximization.logp etc.
using LaplacianExpectationMaximization
using ConcreteStructs, Distributions

@concrete struct HabituatingBiasedCoin
    w
end
HabituatingBiasedCoin() = HabituatingBiasedCoin(Base.RefValue(0.))

function LaplacianExpectationMaximization.initialize!(m::HabituatingBiasedCoin, parameters)
    m.w[] = parameters.w₀
end

LaplacianExpectationMaximization.parameters(::HabituatingBiasedCoin) = (; w₀ = 0., η = 0.)

sigmoid(w) = 1/(1 + exp(-w))

function LaplacianExpectationMaximization.logp(data, m::HabituatingBiasedCoin, parameters)
    LaplacianExpectationMaximization.initialize!(m, parameters)
    η = parameters.η
    logp = 0.
    for yₜ in data
        ρ = sigmoid(m.w[])
        logp += logpdf(Bernoulli(ρ), yₜ)
        m.w[] += η * (yₜ - ρ)
    end
    logp
end

function LaplacianExpectationMaximization.sample(rng, ::Any, m::HabituatingBiasedCoin, ::Any)
    rand(rng) ≤ sigmoid(m.w[])
end</code></pre><h3 id="Generating-data"><a class="docs-heading-anchor" href="#Generating-data">Generating data</a><a id="Generating-data-1"></a><a class="docs-heading-anchor-permalink" href="#Generating-data" title="Permalink"></a></h3><p>Let us generate 5 sequences of 30 steps with this model.</p><pre><code class="language-julia hljs">import LaplacianExpectationMaximization: simulate

model = HabituatingBiasedCoin()
params = (; w₀ = .3, η = .1)
data = [simulate(model, params, n_steps = 30).data for _ in 1:5]</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">5-element Vector{Vector{Bool}}:
 [0, 0, 1, 0, 1, 1, 1, 1, 1, 1  …  0, 0, 0, 0, 1, 1, 0, 0, 1, 0]
 [0, 1, 1, 1, 1, 1, 1, 0, 0, 1  …  1, 1, 1, 0, 1, 0, 1, 1, 0, 1]
 [0, 1, 1, 1, 0, 0, 1, 0, 1, 1  …  1, 1, 1, 0, 0, 1, 1, 1, 1, 0]
 [1, 1, 0, 1, 0, 0, 0, 0, 0, 0  …  1, 1, 0, 1, 0, 1, 1, 1, 1, 1]
 [1, 1, 0, 1, 0, 1, 1, 0, 1, 1  …  1, 0, 1, 0, 1, 1, 0, 1, 1, 1]</code></pre><h3 id="Fitting-a-single-model"><a class="docs-heading-anchor" href="#Fitting-a-single-model">Fitting a single model</a><a id="Fitting-a-single-model-1"></a><a class="docs-heading-anchor-permalink" href="#Fitting-a-single-model" title="Permalink"></a></h3><p>First we check, if gradients are properly computed for our model.</p><pre><code class="language-julia hljs">import LaplacianExpectationMaximization: gradient_logp
gradient_logp(data[1], model, params)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">ComponentVector{Float64}(w₀ = -0.5628259339177734, η = -1.581763311833027)</code></pre><p>If this fails, it is recommended to check that <code>logp</code> does not allocate, e.g. with</p><pre><code class="language-julia hljs">using BenchmarkTools
@benchmark LaplacianExpectationMaximization.logp($(data[1]), $model, $params)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">BenchmarkTools.Trial: 10000 samples with 114 evaluations per sample.
 Range <span class="sgr90">(</span><span class="sgr36"><span class="sgr1">min</span></span> … <span class="sgr35">max</span><span class="sgr90">):  </span><span class="sgr36"><span class="sgr1">761.956 ns</span></span> … <span class="sgr35"> 1.287 μs</span>  <span class="sgr90">┊</span> GC <span class="sgr90">(</span>min … max<span class="sgr90">): </span>0.00% … 0.00%
 Time  <span class="sgr90">(</span><span class="sgr34"><span class="sgr1">median</span></span><span class="sgr90">):     </span><span class="sgr34"><span class="sgr1">766.088 ns              </span></span><span class="sgr90">┊</span> GC <span class="sgr90">(</span>median<span class="sgr90">):    </span>0.00%
 Time  <span class="sgr90">(</span><span class="sgr32"><span class="sgr1">mean</span></span> ± <span class="sgr32">σ</span><span class="sgr90">):   </span><span class="sgr32"><span class="sgr1">772.597 ns</span></span> ± <span class="sgr32">20.531 ns</span>  <span class="sgr90">┊</span> GC <span class="sgr90">(</span>mean ± σ<span class="sgr90">):  </span>0.00% ± 0.00%

   ▄█<span class="sgr34">█</span>▅  ▁<span class="sgr32">▃</span>                                      ▁▂▃▂▂         ▂
  ▅██<span class="sgr34">█</span>█▇▅█<span class="sgr32">█</span>█▅▄▁▄▆▇▇▆▃▄▄▅▄▅▄▄▅▁▃▃▁▄▄▇▇▇▆▄▄▄▃▄▃▃▄▁▆██████▇▇▆▆▆▆▆ █
  762 ns<span class="sgr90">        Histogram: <span class="sgr1">log(</span>frequency<span class="sgr1">)</span> by time</span>       839 ns <span class="sgr1">&lt;</span>

 Memory estimate<span class="sgr90">: </span><span class="sgr33">0 bytes</span>, allocs estimate<span class="sgr90">: </span><span class="sgr33">0</span>.</code></pre><p>We also check if Hessians are properly computed.</p><pre><code class="language-julia hljs">import LaplacianExpectationMaximization: hessian_logp
hessian_logp(data[1], model, params)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">2×2 Matrix{Float64}:
 -3.99137   4.81469
  4.81469  10.7593</code></pre><p>This may fail, if the model is too restrictive in its type parameters.</p><p>If everything works fine we run the optimizer:</p><pre><code class="language-julia hljs">import LaplacianExpectationMaximization: maximize_logp
result = maximize_logp(data[1], model)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">(logp = -6.100566721713756, parameters = (w₀ = -1.9601768165931308, η = -8.377080059530725), extra =  * Status: success

 * Candidate solution
    Final objective value:     6.100567e+00

 * Found with
    Algorithm:     L-BFGS

 * Convergence measures
    |x - x&#39;|               = 0.00e+00 ≤ 0.0e+00
    |x - x&#39;|/|x&#39;|          = 0.00e+00 ≤ 0.0e+00
    |f(x) - f(x&#39;)|         = 0.00e+00 ≤ 0.0e+00
    |f(x) - f(x&#39;)|/|f(x&#39;)| = 0.00e+00 ≤ 0.0e+00
    |g(x)|                 = 6.60e-01 ≰ 1.0e-08

 * Work counters
    Seconds run:   1  (vs limit Inf)
    Iterations:    39829
    f(x) calls:    114079
    ∇f(x) calls:   114079
)</code></pre><p>To inspect the state of the fitted model we can run</p><pre><code class="language-julia hljs">import LaplacianExpectationMaximization: logp_tracked
logp_tracked(data[1], model, result.parameters).history</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">31-element Vector{Main.HabituatingBiasedCoin{Base.RefValue{Float64}}}:
 Main.HabituatingBiasedCoin{Base.RefValue{Float64}}(Base.RefValue{Float64}(-1.9601768165931308))
 Main.HabituatingBiasedCoin{Base.RefValue{Float64}}(Base.RefValue{Float64}(-1.9601768165931308))
 Main.HabituatingBiasedCoin{Base.RefValue{Float64}}(Base.RefValue{Float64}(-0.9260437642968948))
 Main.HabituatingBiasedCoin{Base.RefValue{Float64}}(Base.RefValue{Float64}(1.4507687235605993))
 Main.HabituatingBiasedCoin{Base.RefValue{Float64}}(Base.RefValue{Float64}(-0.13989877280997565))
 Main.HabituatingBiasedCoin{Base.RefValue{Float64}}(Base.RefValue{Float64}(3.7561323702623537))
 Main.HabituatingBiasedCoin{Base.RefValue{Float64}}(Base.RefValue{Float64}(3.56479899228469))
 Main.HabituatingBiasedCoin{Base.RefValue{Float64}}(Base.RefValue{Float64}(3.3342309195438533))
 Main.HabituatingBiasedCoin{Base.RefValue{Float64}}(Base.RefValue{Float64}(3.045930730694639))
 Main.HabituatingBiasedCoin{Base.RefValue{Float64}}(Base.RefValue{Float64}(2.6656659067346706))
 ⋮
 Main.HabituatingBiasedCoin{Base.RefValue{Float64}}(Base.RefValue{Float64}(-2.5148279317601667))
 Main.HabituatingBiasedCoin{Base.RefValue{Float64}}(Base.RefValue{Float64}(-1.888011221037273))
 Main.HabituatingBiasedCoin{Base.RefValue{Float64}}(Base.RefValue{Float64}(-0.7866648188568446))
 Main.HabituatingBiasedCoin{Base.RefValue{Float64}}(Base.RefValue{Float64}(1.8343999339688355))
 Main.HabituatingBiasedCoin{Base.RefValue{Float64}}(Base.RefValue{Float64}(0.6807507751198965))
 Main.HabituatingBiasedCoin{Base.RefValue{Float64}}(Base.RefValue{Float64}(-2.1347335429198906))
 Main.HabituatingBiasedCoin{Base.RefValue{Float64}}(Base.RefValue{Float64}(-1.248719461570828))
 Main.HabituatingBiasedCoin{Base.RefValue{Float64}}(Base.RefValue{Float64}(0.6187150149858869))
 Main.HabituatingBiasedCoin{Base.RefValue{Float64}}(Base.RefValue{Float64}(-2.3138808790799237))</code></pre><p>We can also fit with some fixed parameters.</p><pre><code class="language-julia hljs">result = maximize_logp(data[1], model, fixed = (; η = 0.))
result.parameters</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">ComponentVector{Float64}(w₀ = 0.13353139260903985, η = 0.0)</code></pre><p>or with coupled parameters</p><pre><code class="language-julia hljs">result = maximize_logp(data[1], model, coupled = [(:w₀, :η)])
result.parameters</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">ComponentVector{Float64}(w₀ = -0.053595501797421286, η = -0.053595501797421286)</code></pre><h3 id="Fitting-a-population-model"><a class="docs-heading-anchor" href="#Fitting-a-population-model">Fitting a population model</a><a id="Fitting-a-population-model-1"></a><a class="docs-heading-anchor-permalink" href="#Fitting-a-population-model" title="Permalink"></a></h3><p>Now we fit all data samples with approximate EM, assuming a diagonal normal prior over the parameters.</p><pre><code class="language-julia hljs">import LaplacianExpectationMaximization: PopulationModel
pop_model1 = PopulationModel(model)
result1 = maximize_logp(data, pop_model1)
result1.population_parameters</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">ComponentVector{Float64}(w₀ = 0.3969333853994824, η = -2.953640491034003, population_parameters = (μ = (w₀ = 0.3969333853994824, η = -2.953640491034003), σ = (w₀ = 0.7254354206726409, η = 0.2876797008928165)))</code></pre><p>Let us compare this to a model where all samples are assumed to be generated from the same parameters, i.e. the variance of the normal prior is zero.</p><pre><code class="language-julia hljs">pop_model2 = PopulationModel(model, shared = (:w₀, :η))
result2 = maximize_logp(data, pop_model2)
result2.population_parameters</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">ComponentVector{Float64}(w₀ = 0.5753976529630233, η = -0.06096200383954481, population_parameters = (μ = Float64[], σ = Float64[]))</code></pre><p>To compare the models we look at the approximate BIC</p><pre><code class="language-julia hljs">import LaplacianExpectationMaximization: BIC_int
(model1 = BIC_int(data, pop_model1, result1.population_parameters, repetitions = 1),
 model2 = BIC_int(data, pop_model2, result2.population_parameters))</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">(model1 = [306.3371737905136], model2 = [205.7728519875276])</code></pre><p>We see that the second model without variance of the prior has the lower BIC. This is not surprising, given that the data was generated with identical parameters.</p></article><nav class="docs-footer"><a class="docs-footer-nextpage" href="rl/">Fitting a Q-Learner »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.9.0 on <span class="colophon-date" title="Thursday 27 March 2025 16:28">Thursday 27 March 2025</span>. Using Julia version 1.11.4.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
